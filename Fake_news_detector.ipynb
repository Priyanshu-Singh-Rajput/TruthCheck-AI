{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu-Singh-Rajput/TruthCheck-AI/blob/main/Fake_news_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_JcAe-bdXmy"
      },
      "source": [
        "Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_7aKFELKb3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXnDy7rXd_O1"
      },
      "source": [
        "Step 1:installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlwKSHeAeKG-"
      },
      "outputs": [],
      "source": [
        "# Core LangChain\n",
        "!pip install -q langchain\n",
        "\n",
        "#NLP Tools\n",
        "!pip install -q spacy\n",
        "\n",
        "#IBM Watson NLP (for NLU or sentiment)\n",
        "!pip install -q ibm-watson\n",
        "\n",
        "#Download SpaCy model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "#Download google gen ai\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SZCFSs8eXIL"
      },
      "source": [
        "Step 2:Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogNo8vzKf90M"
      },
      "outputs": [],
      "source": [
        "# Core utilities\n",
        "import os\n",
        "import re\n",
        "\n",
        "# LangChain modular tools\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# NLP processing\n",
        "import spacy\n",
        "\n",
        "# IBM Watson NLU\n",
        "from ibm_watson import NaturalLanguageUnderstandingV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "from ibm_watson.natural_language_understanding_v1 import Features, KeywordsOptions, EntitiesOptions\n",
        "\n",
        "#google gen ai\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZVYmCBlgJC7"
      },
      "source": [
        "Step 3:Setup API KEYS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8qTy5-DhFH_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Securely collect API credentials\n",
        "import getpass\n",
        "\n",
        "watson_api_key = getpass.getpass(\"ðŸ” Enter your IBM Watson API Key: \")\n",
        "watson_url = input(\"ðŸŒ Enter your IBM Watson Service URL: \").strip()\n",
        "gemini_api_key=getpass.getpass(\"ðŸ” Enter your Gemini API Key: \")\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
        "\n",
        "# Configure IBM Watson NLU client\n",
        "authenticator = IAMAuthenticator(watson_api_key)\n",
        "nlu = NaturalLanguageUnderstandingV1(\n",
        "    version=\"2021-08-01\",\n",
        "    authenticator=authenticator\n",
        ")\n",
        "nlu.set_service_url(watson_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urj_5fnUh0eG"
      },
      "source": [
        "Step 4: Function to take input from User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJlaeOcsntrY"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import Textarea, Button, VBox, Output\n",
        "out = Output()\n",
        "ta = Textarea(placeholder=\"Paste your text hereâ€¦\", layout={'width':'500px','height':'200px'})\n",
        "btn = Button(description=\"Submit\")\n",
        "def on_submit(b):\n",
        "    with out:\n",
        "        print(\"Received:\", ta.value[:100], \"â€¦\")\n",
        "btn.on_click(on_submit)\n",
        "display(VBox([ta, btn, out]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlBIPjQdoBr9"
      },
      "source": [
        "Step 5: Agent to anaylize Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOr3Nyxqop0d"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def preprocess_merged(text: str) -> dict:\n",
        "    # 1. Remove emojis (but keep punctuation!)\n",
        "    raw_text = re.sub(r\"[\\U00010000-\\U0010ffff]\", \"\", text)\n",
        "\n",
        "    # 2. Run spaCy on raw text (for sentence detection + entities)\n",
        "    doc = nlp(raw_text)\n",
        "\n",
        "    # Lemmatized, lowercase, no stop words â€” for Watson input\n",
        "    clean_tokens = [tok.lemma_.lower()\n",
        "                    for tok in doc\n",
        "                    if tok.is_alpha and not tok.is_stop]\n",
        "    clean_text = \" \".join(clean_tokens)\n",
        "\n",
        "    # Extract entities from spaCy\n",
        "    spacy_entities = {(ent.text, ent.label_) for ent in doc.ents}\n",
        "\n",
        "    # 3. Watson NLU enrichment (on clean text)\n",
        "    resp = nlu.analyze(\n",
        "        text=clean_text,\n",
        "        features=Features(\n",
        "            keywords=KeywordsOptions(limit=10),\n",
        "            entities=EntitiesOptions(limit=10)\n",
        "        )\n",
        "    ).get_result()\n",
        "    watson_keywords = {kw[\"text\"] for kw in resp[\"keywords\"]}\n",
        "    watson_entities = {(ent[\"text\"], ent[\"type\"]) for ent in resp[\"entities\"]}\n",
        "\n",
        "    # 4. Merge & dedupe entities\n",
        "    merged_keywords = list(spacy_entities and watson_keywords or watson_keywords)\n",
        "    merged_entities = list(watson_entities.union(spacy_entities))\n",
        "\n",
        "    return {\n",
        "        \"raw_text\": raw_text,         # â† use this for claim extraction\n",
        "        \"clean_text\": clean_text,     # â† use this for Watson/embeddings\n",
        "        \"keywords\": merged_keywords,\n",
        "        \"entities\": merged_entities\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjfUzSyAuZBO"
      },
      "outputs": [],
      "source": [
        "result = preprocess_merged(ta.value)\n",
        "print(\"Raw Text:\", result[\"raw_text\"])\n",
        "print(\"Clean Text:\", result[\"clean_text\"])\n",
        "print(\"Keywords:\", result[\"keywords\"])\n",
        "print(\"Entities:\", result[\"entities\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL1vCeRaojUx"
      },
      "source": [
        "Step 6: Claim Extraction Agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_claims(text):\n",
        "    doc = nlp(text)\n",
        "    claims = []\n",
        "    for sent in doc.sents:\n",
        "        if sent[-1].text != '?':  # skip questions\n",
        "            if any(tok.dep_ == 'ROOT' and tok.pos_ == 'VERB' for tok in sent):\n",
        "                claims.append(sent.text.strip())\n",
        "    return claims\n",
        "claim_extraction_tool = Tool(\n",
        "    name=\"Claim Extraction Agent\",\n",
        "    func=extract_claims,\n",
        "    description=\"Extracts factual claim-like sentences from cleaned text.\"\n",
        ")"
      ],
      "metadata": {
        "id": "-bf65_u5Oiw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly use the function to check output immediately\n",
        "claims = extract_claims(result[\"raw_text\"])\n",
        "\n",
        "print(\"ðŸ” Extracted Claims:\")\n",
        "for i, claim in enumerate(claims, 1):\n",
        "    print(f\"{i}. {claim}\")"
      ],
      "metadata": {
        "id": "vzW8wOuTPPJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Search and Verdict Agent"
      ],
      "metadata": {
        "id": "YYn4yuNA-06R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "def verify_claim_with_gemini(claim: str, keywords: list[str]) -> dict:\n",
        "    search_terms = f\"{claim} {' '.join(keywords)}\"\n",
        "    prompt = f\"\"\"\n",
        "You are a fact-checking assistant.\n",
        "\n",
        "Given the following claim and context keywords, search for up-to-date information and decide if the claim is:\n",
        "-  Supported\n",
        "-  Refuted\n",
        "-  Not Enough Evidence\n",
        "\n",
        "Respond in this format:\n",
        "Verdict: <one of the above>\n",
        "Justification: <one short paragraph based on your reasoning>\n",
        "\n",
        "Claim: \"{claim}\"\n",
        "Keywords: {', '.join(keywords)}\n",
        "\n",
        "Please begin.\n",
        "\"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return {\n",
        "        \"claim\": claim,\n",
        "        \"verdict_response\": response.text\n",
        "    }\n",
        "\n",
        "# Example use with outputs from previous steps:\n",
        "claims = extract_claims(ta.value)\n",
        "keywords = list(result[\"keywords\"])\n",
        "\n",
        "# Run verification\n",
        "verified_claims = [verify_claim_with_gemini(claim, keywords) for claim in claims]\n",
        "\n",
        "# Print results\n",
        "for v in verified_claims:\n",
        "    print(\"\\nðŸ§¾ Claim:\", v[\"claim\"])\n",
        "    print(v[\"verdict_response\"])"
      ],
      "metadata": {
        "id": "jakf-Dhd-10z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}